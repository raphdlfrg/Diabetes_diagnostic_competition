{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5j7NbO9QHyAo",
        "outputId": "f5ebf572-9f03-467f-efab-d0cb7874580b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2: 100%|██████████| 6342/6342 [01:47<00:00, 58.82batch/s, loss=0.421]\n",
            "Epoch 2/2: 100%|██████████| 6342/6342 [01:49<00:00, 57.89batch/s, loss=0.326]\n"
          ]
        }
      ],
      "source": [
        "from ast import mod\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from time import time\n",
        "import sklearn.preprocessing\n",
        "import sklearn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "\n",
        "x_train = pd.read_csv(\"train.csv\")\n",
        "x_test = pd.read_csv(\"test.csv\")\n",
        "y_train = pd.read_csv(\"labels.csv\")\n",
        "\n",
        "\n",
        "#x_train = x_train.sample(frac=0.5, random_state=42)\n",
        "#y_train = y_train.loc[x_train.index]\n",
        "\n",
        "\n",
        "x_train, x_test = x_train.drop(columns=['Age_Group']), x_test.drop(columns=['Age_Group'])\n",
        "\n",
        "\n",
        "def feature_encoding(X):\n",
        "\n",
        "    non_numerical_columns_names = X.select_dtypes(exclude=['number']).columns\n",
        "\n",
        "    for column in non_numerical_columns_names:\n",
        "        le = LabelEncoder()\n",
        "        X[column] = le.fit_transform(X[column])\n",
        "\n",
        "    return X\n",
        "\n",
        "def normalize_features(X_train, X_test):\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
        "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
        "\n",
        "    return X_train_scaled, X_test_scaled\n",
        "\n",
        "\n",
        "x_train, x_test = feature_encoding(x_train), feature_encoding(x_test)\n",
        "\n",
        "x_train_scaled, x_test_scaled = normalize_features(x_train, x_test)\n",
        "\n",
        "\n",
        "pca = PCA(n_components=15)\n",
        "\n",
        "\n",
        "x_train_scaled = pca.fit_transform(x_train_scaled, y=None)\n",
        "x_train_scaled = pd.DataFrame(\n",
        "    x_train_scaled,\n",
        "    columns=[f'PC{i+1}' for i in range(pca.n_components_)],\n",
        "    index=x_train.index)\n",
        "\n",
        "\n",
        "x_test_scaled = pca.transform(x_test_scaled)\n",
        "x_test_scaled = pd.DataFrame(\n",
        "    x_test_scaled,\n",
        "    columns=[f'PC{i+1}' for i in range(pca.n_components_)],\n",
        "    index=x_test.index\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device=torch.device(\"cuda\")\n",
        "else:\n",
        "    device=torch.device(\"cpu\")\n",
        "\n",
        "X_train=torch.tensor(x_train_scaled.to_numpy(),dtype=torch.float32).to(device)\n",
        "Y_train = torch.tensor(y_train[\"Diabetes_binary\"].to_numpy(), dtype=torch.long).to(device)\n",
        "X_test=torch.tensor(x_test_scaled.to_numpy(),dtype=torch.float32).to(device)\n",
        "\n",
        "\n",
        "train_dataset=TensorDataset(X_train,Y_train)\n",
        "train_loader=DataLoader(train_dataset,batch_size=32,shuffle=True)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "        super(MLP, self).__init__()\n",
        "        layers=[]\n",
        "        for i in range(len(hidden_size)):\n",
        "          layers.append(nn.Linear(input_size, hidden_size[i]))\n",
        "          layers.append(nn.ReLU())\n",
        "          input_size=hidden_size[i]\n",
        "\n",
        "        layers.append(nn.Linear(hidden_size[-1], len(np.unique(y_train))))\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "\n",
        "param={\n",
        "    'hidden_size':[128,64,32],\n",
        "    'epochs':25,\n",
        "    'learning_rate':0.001,\n",
        "    'batch_size':32,\n",
        "    'optimizer':'adam'\n",
        "}\n",
        "\n",
        "\n",
        "model=MLP(X_train.shape[1],param['hidden_size'],).to(device)\n",
        "\n",
        "optimizer=optim.Adam(model.parameters(),lr=param['learning_rate'])\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "for epoch in range(param['epochs']):\n",
        "\n",
        "  model.train()\n",
        "  total_loss=0.0\n",
        "  with tqdm(train_loader,unit=\"batch\") as tepoch:\n",
        "    for x,y in tepoch:\n",
        "      optimizer.zero_grad()\n",
        "      output=model(x)\n",
        "      loss=criterion(output,y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      tepoch.set_description(f\"Epoch {epoch+1}/{param['epochs']}\")\n",
        "      total_loss+=loss.item()\n",
        "      tepoch.set_postfix(loss=total_loss/(tepoch.n+1))\n",
        "\n",
        "\n",
        "\n",
        "  model.eval()\n",
        "  pred=[]\n",
        "  with torch.no_grad():\n",
        "    for x in X_test:\n",
        "      x = x.unsqueeze(0)\n",
        "      output=model(x)\n",
        "      _,predicted=torch.max(output,1)\n",
        "      pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "y_test_pred = pd.DataFrame(pred, columns=['Diabetes_binary'], index=x_test['Unnamed: 0'])\n",
        "\n",
        "y_test_pred.index.name = 'index'\n",
        "\n",
        "y_test_pred.to_csv(\"test_predictions.csv\", index=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}